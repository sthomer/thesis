\section{Context, Problems, \& Justifications}
\label{section:context-problems-justifications}

In the past few years, Artificial Intelligence has experienced an unparalleled resurgence in academia and popular culture. Due to the massive scale of data and processing power made available by the internet, the problems of sparse data and slow CPUs that once plagued Machine Learning techniques like Deep Learning \citep{lecun2015deep} and Reinforcement Learning \citep{sutton1998introduction} have disappeared, yielding highly effective methods of analyzing data. However, with all of this recent progress, fundamental questions have reemerged as well. The “interpretability problem” \citep{chalmers1996conscious} – that the internal workings of a neural network or other ML technique are so unintelligible as to be meaningless to a human observer – has resulted in a lack of trust and understanding in AI systems. Though neural networks are effective in generating accurate results, explanations of their inner parametrization are at best ad-hoc. This problem of AI Explainability yields the question “Can popular AI methods such as Neural Networks and Reinforcement Learning be embedded with semantics? Do these systems have a sort of natural semantics, and hence explainability?”

When examining this question of explainability, and therefore semantics, the natural first place to look is how humans go about explaining. Though the process of explaining a given concept is simple, usually through a series of interdependent reasons or set of examples, the way that these reasons and examples exist in the mind is a mystery. It seems that our internal voice asks for an explanation, and without much effort, our non-conscious mind searches our memory for
representations and relations that solve that question \citep{baars1993cognitive}. So though we generally know when an explanation is satisfactory, intuitive, or even deep, we’re not sure of the manner in which the content of that explanation is represented and recruited. “How does the mind represent information and relations? How do we learn these deep, abstract representations from a relatively paltry amount of perceptual data?” \citep{quine1969ontological}

The Information Dynamics of Thinking theory (IDyOT) \citep{wiggins2018creativity} is a cognitive architecture that seeks to answer these questions with a minimal set of assumptions and processes, drawing inspiration from information theory and conceptual space theory.  This thesis is an explication and implementation of IDyOT, with the goal of providing evidence for its theoretical processes and claims.
