\section{Information Theory}
\label{section:information-theory}

Shannon's information theory \citep{shannon1948mathematical}, though originally aimed at providing a theoretical foundation to signal processing and communication, has become foundational in all aspects regarding information.  According to information theory, the amount of information in a signal can be thought of as the amount of surprise at seeing a given quantity in that signal.  Put more simply, suppose you come in late to work one morning, and upon arriving into the office, your boss tells you about the weather. No information was gleaned from the conversation, since you already knew the weather from being outside.  If instead, upon entering the office late, your boss fires you on the spot, you would be very surprised, learning something unexpected, and gaining a lot of information.

\subsection{Entropy}
\label{subsection:entropy}

Entropy $H$ is the measure of information in a probability distribution.  If applied to a discrete signal of symbols, it can be used to represent the amount of information of the signal (equation \ref{equation:entropy}), by taking it over the frequencies of each symbol $s$ of the alphabet $A$.  Entropy is then the number of bits required to represent each symbol without ambiguity.

\begin{equation}
  \label{equation:entropy}
  H(A) = - \sum_{s \in A} p(s) \log_2 p(s)
\end{equation}

\subsection{Information Content}
\label{subsection:information-content}

Though entropy is useful in describing the quantity of information of an entire distribution, to find the information content $h(s)$ of a single symbol in an alphabet, one must use equation \ref{equation:information-content} below \citep{mackay2003information}. Since some symbols are less likely to occur than others, observing them gives more information than more likely ones, which is reflected here.

\begin{equation}
  \label{equation:information-content}
  h(s) = - \log_2 p(s)
\end{equation}

Whereas information content looks at a symbol in isolation, conditional entropy looks at symbols in pairs.  Conditional entropy is equivalent to the information content of a symbol, given that another symbol is known.  If we are referring to a stream of symbols, the conditional entropy would be the information content of a symbol given that we just saw the previous symbol.

\begin{equation}
  \label{equation:conditional-entropy}
  H(s|t) = -\log_2 p(s|t)
\end{equation}


